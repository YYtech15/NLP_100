## Word2Vec
### 概要
- 自然言語処理（NLP）の分野で広く使用される技術で、単語をベクトルに変換する手法
- 単語の意味を数値ベクトルとして表現することで、コンピュータが自然言語を理解しやすくする

### ****Word2Vecの基本概念****

- 単語を高次元のベクトル空間にマッピングすることで、単語間の意味的な類似性を捉える
 → 類似した意味を持つ単語がベクトル空間内で近接するようになる

#### Word2Vecの主な２つのモデル

- **Continuous Bag of Words (CBOW)モデル**
  - 文脈から中心となる単語を予測するモデル。
  - 周囲の単語（コンテキスト）から中心の単語を予測することで、単語のベクトルを学習。
    - （例）「私は図書館で本を借りて読みました」という文があった場合、「私は」「図書館で」「借りて」「読みました」が文脈となり、「本」が中心の単語となります。
- **Skip-Gramモデル**
  - 中心の単語から周囲の単語を予測するモデル。
  - 中心の単語からその周囲にある単語を予測することで、単語のベクトルを学習。
  - （例）「本」が中心の単語であれば、「私は」「図書館で」「借りて」「読みました」が予測対象の単語となる

### ****Word2Vecの仕組み****

浅い2層のニューラルネットワークを使用して単語のベクトルを学習。

#### 具体的なベクトル作成プロセス

1. **入力データの準備**：大量のテキストデータ（コーパス）を用意。このコーパスには、学習対象となる単語が含まれる。
2. **モデルの学習**：CBOWまたはSkip-Gramモデルを使用して、単語のベクトルを学習。これにより、各単語が高次元のベクトル空間内で表現される。
3. **ベクトルの生成**：学習が完了すると、各単語に対応するベクトルが生成。このベクトルは、単語の意味的な特徴を捉えたもの。

### ****ベクトルの生成と利用****

- **類似度の計算**
  - ベクトル間のコサイン類似度を計算することで、単語間の意味的な類似性を定量化可能。
  - （例）「王様」-「男」+「女」=「女王」のような計算が可能
- **クラスタリング**
  - 意味が似ている単語がベクトル空間内で近接するため、クラスタリングによって単語のグループを形成可能。

### ****Word2Vecの応用例****
- **対話型AI**
  - Word2Vecを用いて、単語やフレーズの意味をベクトルで表現し、自然な会話を生成。
  - （例）カスタマーサービスや仮想アシスタントなど
- **機械翻訳**：異なる言語間での単語の関連性や意味を捉えることで、より正確な翻訳を実現。
- **口コミやレビューの分析**：テキストデータのパターンや関連性を分析し、製品やサービスの質、ユーザーの感情、トレンドなどの洞察を提供。
- **検索エンジン**：単語の意味的な関連性を理解し、より適切で精度の高い検索結果を提供。
- **商品推奨システム**：ユーザーのテキストデータから嗜好を分析し、パーソナライズされた推薦を行う。